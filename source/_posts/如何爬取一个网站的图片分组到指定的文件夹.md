---
layout: post
title: 如何爬取一个网站的图片分组到指定的文件夹
toc: true
reward: true
date: 2020-12-09 09:37:30
tags: [Python3.7,爬虫]
categories: [Python]
---


###  开发工具：

* Mac电脑    

* PyCharm

* Python3.7

* 工具包 **bs4、os、requests**

#### 网页信息

![img](https://mmbiz.qpic.cn/mmbiz_png/WGyNiboAjLV7cHtVA3p3bYOrDpxiaSCGicr8QmibfJePDfkanj24gu7Qjka5LEicltPQbZIKt7XSb94t43vG22UJxtw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
<!-- more -->

*从上面这张图我们可以看出，一页有多套图，这个时候我们就要想怎么把每一套图分开存放（后边具体解释）*
通过分析，所有信息在页面中都可以拿到，我们就不考虑异步加载，那么要考虑的就是分页问题了，通过点击不同的页面，很容易看清楚分页规则

![img](https://mmbiz.qpic.cn/mmbiz_png/WGyNiboAjLV7cHtVA3p3bYOrDpxiaSCGicroN6T3CN0vmAKkOdPIZ6GsL2LxAicSXDROH4NGvlssb2iaV7vT3I9PZgQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

*很容易明白分页URL的构造，图片链接都在源码中，就不做具体说明了明白了这个之后就可以去写代码抓图片了*

#### 存图片的思路

因为要把每一套图存入一个文件夹中（os模块），文件夹的命名我就以每一套图的URL的最后的几位数字命名，然后文件从文件路径分隔出最后一个字段命名,具体看下边的截图。

![img](https://mmbiz.qpic.cn/mmbiz_png/WGyNiboAjLV7cHtVA3p3bYOrDpxiaSCGicrs1YO4cw4sWDegXqZ54ujz7VZ1fZds5TRIlUicRZqpJ0OzzPceeW71ng/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/WGyNiboAjLV7cHtVA3p3bYOrDpxiaSCGicrR89T7BTesr71ibH0iaYWZU7XnXEpcu4FdzmamOPib0ZefkEsUv8Ykx9pg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

这些搞明白之后，接下来就是代码了（可以参考我的解析思路，只获取了30页作为测试）**全部源码**,下面的源码有原文中的有点不一样。

```python
# -*- coding:utf-8 -*-
from bs4 import BeautifulSoup
import os
import requests


class doutuSpider(object):
    headers = {
        "user-agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36"}

    def get_url(self, url):
        data = requests.get(url, headers=self.headers)
        soup = BeautifulSoup(data.content, 'lxml')
        totals = soup.findAll("a", {"class": "list-group-item"})
        for one in totals:
            sub_url = one.get('href')
            global path
            path = "/Users/mark/Desktop/Mark/doutu/" + sub_url.split('/')[-1]
            os.makedirs(path, exist_ok=True)
            try:
                self.get_img_url(sub_url)
            except:
                pass

    def get_img_url(self, url):
        data = requests.get(url, headers=self.headers)
        soup = BeautifulSoup(data.content, 'lxml')
        totals = soup.find_all('div', {'class': 'artile_des'})
        for one in totals:
            img = one.find('img')
            try:
                sub_url = img.get('src')
            except:
                pass
            finally:
                # http://ww1.sinaimg.cn/large/9150e4e5gy1g59ef4u9edg206o06ogni.gif
                print(sub_url)
                try:
                    self.get_img(sub_url)
                except:
                    pass

    def get_img(self, url):
        filename = url.split('/')[-1]
        global path
        img_path = path + '/' + filename
        print('正在下载----', url)
        print('存储地址为----', img_path)
        try:
            img = requests.get(url, headers=self.headers)
            with open(img_path, 'wb') as f:
                f.write(img.content)
                f.close()
        except requests.RequestException:
            print('请求图片出错', url)

    def create(self):
        for count in range(1, 2):
            url = 'https://www.doutula.com/article/list/?page={}'.format(count)
            print('开始下载第{}页'.format(count))
            self.get_url(url)


if __name__ == '__main__':
    doutu = doutuSpider()
    doutu.create()

```

#### 结果

![img](https://mmbiz.qpic.cn/mmbiz_png/WGyNiboAjLV7cHtVA3p3bYOrDpxiaSCGicrTamKCbHGDr8Z4sGGZbnLOUGMOGickw2Raa1HVowjrgKDib0wwXmqKQmw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/WGyNiboAjLV7cHtVA3p3bYOrDpxiaSCGicrhzHDdct4Y6FZ4melwBj3NdY5lKUicvrk9ib5ZNFdN140PicmQiaSBxRMwQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/WGyNiboAjLV7cHtVA3p3bYOrDpxiaSCGicrMWzicYkDdR1nC7viaw97CTmzynWs7JJexuF297FWdChrJNNAn25lNSAg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

#### 总结 

总的来说，这个网站结构相对来说不是很复杂，大家可以参考一下，爬一些有趣的

原创作者：loading_miracle，原文链接:
https://www.jianshu.com/p/88098728aafd